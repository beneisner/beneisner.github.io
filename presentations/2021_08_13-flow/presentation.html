---
layout: presentation
title: Flow-based Policy Transfer
permalink: /flow
published: true
---

<style>
    .container {
        display: flex;
    }
    .col {
        flex: 1;
    }
    .given {
        font-size: 0.7em;
        line-height: .4em;
    }
</style>

# Policy Transfer by Imagining Flow <!-- .element: class="r-fit-text" -->

#### Ben Eisner

<p style="font-size: .7em;"><a href=https://beisner.me/flow>https://beisner.me/flow</a></p>


-----

## A Motivating Example

#### you might remember this one! <!-- .element: class="fragment" data-fragment-index="1" -->

##### (but I promise it's a bit different this time...) <!-- .element: class="fragment" data-fragment-index="2" -->

---

<img src="presentations/2021_08_13-flow/lighter_clean.png">

How does this object work? <!-- .element: class="fragment" data-fragment-index="1" -->

---

<div class="container">
    <div class="col">
        <img src="presentations/2021_08_13-flow/lighter_clean.png" class="fragment" height=300 data-fragment-index="1">
    </div>
    <div class="col">
        <img src="presentations/2021_08_13-flow/lighter_segment.svg" class="fragment" height=300 data-fragment-index="2">
    </div>
    <div class="col">
        <img src="presentations/2021_08_13-flow/lighter_w_flow.svg" height="300" class="fragment" data-fragment-index="3">
    </div>
</div>

<div class="r-stack">
  <div class="fragment current-visible" data-fragment-index="1">For now, let's just ask:<p><b>How does this object move?</b></p></div>
  <div>
    <p class="fragment" data-fragment-index="2">Maybe we notice individual parts...</p>
    <p class="fragment" data-fragment-index="3">...and imagine <u>what kinds of motions are possible.</u></p>
    <p class="fragment" data-fragment-index="4">(we can answer this because we've seen lighters before)</p>
  </div>
</div>

---

<div class="container">
    <div class="col">
        <img src="presentations/2021_08_13-flow/lighter_w_flow.svg" class="fragment" height=300 data-fragment-index="1">
    </div>
    <div class="col">
        <img src="presentations/2021_08_13-flow/arrow.svg" class="fragment" width=200 data-fragment-index="2">
    </div>
    <div class="col">
        <div class="r-stack fragment" data-fragment-index="3">
            <img src="presentations/2021_08_13-flow/thumbsup.svg" height="335">
            <!-- <img src="presentations/2021_08_13-flow/lighter_w_flow.svg" height="300"  > -->
        </div>
    </div>
</div>

<div class="r-stack">
    <div class="fragment current-visible" data-fragment-index="1">
        Informally, let's call this visual affordance <mark>"imagined flow"</mark>.
    </div>
    <div class="current-visible" >
        <p class="fragment" data-fragment-index="2">
            With <mark>imagined flow</mark> flow alone, 
        </p>
        <p class="fragment" data-fragment-index="3">
            we can derive a reasonably good manipulation policy.
        </p>
    </div>
    <!-- <p class="fragment" data-fragment-index="2">With this imagined flow alone,</p> -->
    <!-- <p class="fragment" data-fragment-index="2">Maybe we notice individual parts...</p>
    <p class="fragment" data-fragment-index="3">...and imagine <u>what kinds of motions are possible.</u></p>
    <p class="fragment" data-fragment-index="4">(we can answer this because we've seen lighters before)</p> -->
</div>
  
---

Two assumptions we'd like to relax:

1. We have an object model we can segment into parts. <!-- .element: class="fragment" data-fragment-index="1" -->

2. <u>We already have an understanding of how we want this object to move.</u> <!-- .element: class="fragment" data-fragment-index="2" -->

---

Relaxation 1: No need for object model or segmentation.

<div class="container">
    <div class="col">
        <img src="presentations/2021_08_13-flow/lighter_segment.svg" class="fragment" height=300 data-fragment-index="1">
    </div>
    <div class="col">
        <img src="presentations/2021_08_13-flow/arrow.svg" class="fragment" width=200 data-fragment-index="2">
    </div>
    <div class="col">
        <img src="presentations/2021_08_13-flow/lighter_pc.svg" height="300" class="fragment" data-fragment-index="3">
    </div>
</div>

<div class="fragment">
<b>Geometry seems relevant to this task</b>, so we choose to operate on <mark>unlabeled point clouds</mark> 
</div>

(colors here are for visualization, not provided) <!-- .element: class="fragment" -->

---

Relaxation 2: No need to understand the task completely - we can use a motion demonstration from another object!

<img src="presentations/2021_08_13-flow/lighter_pc_flow_transfer.svg" class="fragment" height=300>

<div class="fragment">
In other words, <mark>observe flow on a source object</mark> during a demonstration, and try and <u>transfer that characteristic flow</u> to another object. 
</div>
---

## <u>Problem Statement</u>

### Given <!-- .element: class="fragment" -->

<div class="container">
    <div class="col given fragment">
        <u>Source Object $A$:</u>
        <ul>
            <li>a source point cloud $P_A$</li>
            <li>a ground-truth flow $F_A$</li>
        </ul>
    </div>
    <div class="col given fragment">
        <u>Destination object $B$:</u>
        <ul>
            <li>a destination point cloud $P_B$</li>
        </ul>
    </div>
</div>

<br>

### Task <!-- .element: class="fragment" -->

<div class="fragment">
<mark>Transfer</mark> flow $F_A$ to object $B$ to produce flow $F_B$
so that the object deforms in a <u>characteristically similar way</u>.
</div>

---

In other words, we want to learn a function

`$$\tilde{F_B} = f_{\theta}(P_A, P_B, F_A)$$`

where, for some ground truth flow `$F_{B|A}$` we minimize:

`$$\mathbb{E}\left[D(F_{B|A}, \tilde{F_B})\right]$$`

for some similarity function `$D$`.

-----


### Related Work

---

<u>Predicting articulation structure.</u> <!-- .element: class="r-fit-text" -->

Screw-Net

Category-Level Articulated Object Pose Estimation

Visual Identification of Articulated Object Parts

---
### <u>Screw-Net</u>

<img src="presentations/2021_08_13-flow/screw_net.png" height=300>

**Key idea: unify articulations under screw theory, and estimate screw parameters from video.**

<div style="font-size: 0.3em; line-height: 1.2em;">
    <u>ScrewNet: Category-Independent Articulation Model Estimation From Depth Images Using Screw Theory</u> <br>
    Ajinkya Jain, Rudolf Lioutikov, Caleb Chuck, Scott Niekum <br>
    <a href="https://arxiv.org/abs/2008.10518">https://arxiv.org/abs/2008.10518</a>
</div>
---

### <u>Category-Level Articulated Object Pose Estimation</u>


<div style="font-size: 0.3em; line-height: 1.2em;">
    <u>Category-Level Articulated Object Pose Estimation</u> <br>
    Xiaolong Li, He Wang, Li Yi, Leonidas Guibas, A. Lynn Abbott, Shuran Song <br>
    <a href="https://arxiv.org/abs/1912.11913">https://arxiv.org/abs/1912.11913</a>
</div>
---

<u>Learning manipulation affordances for articulated objects.</u> <!-- .element: class="r-fit-text" -->

Act the Part: Learning Interaction Strategies for Articulated Object Part Discovery

*** This is about learning articulated parts, but in a lot of ways it's learning affordances for where to manipulate articulated objects.

Where2Act: From Pixels to Actions for Articulated 3D Objects

-----

### Method

---

### Architecture

<img src="presentations/2021_08_13-flow/method_diagram.svg">


---

### Architecture

<ul>
    <li class="fragment"><mark>FlowNet</mark> encodes the source object geometry and the object flow (i.e. from a demonstration)</li>
    <li class="fragment">
        <mark>TransferNet</mark>
        <ul>
            <li>first, encodes the geometry of the destination object</li>
            <li>second, injects the FlowNet embedding at the lowest layer</li>
            <li>third, reconstructs the dstination flow</li>
        </ul>
    </li>
</ul>

---

### Dataset: PartNet-Mobility

<video autoplay loop height="300">
    <source src="./presentations/2021_08_13-flow/data_demo.mp4" type="video/mp4">
</video>

<ul>
    <li class="fragment">50+ object categories, 2000+ objects</li>
    <li class="fragment">3D models for each object</li>
    <li class="fragment"><u>Models annotated with articulations, so demonstrations can be generated.</u></li>
</ul>

---

### Training

<div style="font-size: 0.7em;">
Choose a single training class and a single articulation type (i.e. faucets/handles)

<u>For each training step:</u>
<ul>
    <li class="fragment">Select a pair of objects $A,B$ which each have 1+ instance of the articulation type in question</li>
    <li class="fragment">On each object, generate flows $F_A$, $F_{B|A}$ that correspond to articulation motions in the same direction and with the same magnitude </li>
    <li class="fragment">Construct tuple ($P_A, F_A, P_B, F_{A|B}$)</li>
    <li class="fragment">Compute flow: $$\tilde{F}_{B|A} = f_{\theta, \phi}(P_A, F_A, P_B)$$</li>
    <li class="fragment">Compute loss: $$L_{\text{MSE}} = \frac{1}{N}\sum_{f_{B,i}, \tilde{f}_{B,i} \in (F_{B|A}, \tilde{F}_{B|A})}{|| f_{B,i} - \tilde{f}_{B,i} ||^2}$$</li>
</ul>
</div>
-----


### Results

-----

Vertical slides are separated by `- - -`

<iframe src="presentations/2021_08_13-flow/1_gt.html" height="400" width="800"></iframe>
---

Vertical slides are separated by `- - -` .




---

### Here's an example

`$$ f(x) = \pi $$`

---

Vertical slides are separated by `- - -` ..

-----

# title
## skills
* one
* two
* three

## heading
8. three
1. one
1. two


-----


# Questions

-----

## made with

#### [reveal.js](https://github.com/hakimel/reveal.js)

#### [excalidraw](https://github.com/excalidraw/excalidraw)